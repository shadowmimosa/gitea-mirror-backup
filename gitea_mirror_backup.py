#!/usr/bin/env python3
"""
Gitea Docker é•œåƒä»“åº“å¤‡ä»½ç³»ç»Ÿ
é€‚ç”¨äº: Docker è¿è¡Œçš„ Gitea
åŠŸèƒ½: æ¯æ—¥å¿«ç…§ + æ¯å‘¨æ±‡æ€»æŠ¥å‘Š
"""

import sys
import shutil
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
import logging
from typing import Optional, List
import argparse

# å¯¼å…¥é…ç½®åŠ è½½å™¨
try:
    from src.config_loader import Config
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥é…ç½®åŠ è½½å™¨")
    print("è¯·ç¡®ä¿ src/config_loader.py å­˜åœ¨")
    sys.exit(1)

# å¯¼å…¥é€šçŸ¥ç³»ç»Ÿï¼ˆå¯é€‰ï¼‰
try:
    from src.notifier import NotificationManager

    NOTIFIER_AVAILABLE = True
except ImportError:
    NotificationManager = None
    NOTIFIER_AVAILABLE = False


# ============ æ—¥å¿—é…ç½® ============
def setup_logging(config_instance: Config):
    """è®¾ç½®æ—¥å¿—"""
    # ç¡®ä¿æ—¥å¿—æ–‡ä»¶å­˜åœ¨
    log_file = Path(config_instance.LOG_FILE)
    log_file.parent.mkdir(parents=True, exist_ok=True)
    log_file.touch(exist_ok=True)

    # è·å–æ—¥å¿—çº§åˆ«
    log_level = getattr(logging, config_instance.LOG_LEVEL.upper(), logging.INFO)

    # è·å–æ—¥å¿—æ ¼å¼
    loader = config_instance.get_loader()
    log_format = loader.get('logging.format', '[%(asctime)s] %(message)s')
    date_format = loader.get('logging.date_format', '%Y-%m-%d %H:%M:%S')

    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=log_level,
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler(config_instance.LOG_FILE),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


# å…¨å±€å˜é‡ï¼Œç¨ååˆå§‹åŒ–
logger = None
config = None
notifier = None


# ============ å·¥å…·å‡½æ•° ============
def run_command(
    cmd: List[str], check=True, capture_output=True
) -> subprocess.CompletedProcess:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(
            cmd, check=check, capture_output=capture_output, text=True
        )
        return result
    except subprocess.CalledProcessError as e:
        if check:
            logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {' '.join(cmd)}")
            logger.error(f"é”™è¯¯è¾“å‡º: {e.stderr}")
            raise
        return e


def check_docker_container() -> bool:
    """æ£€æŸ¥ Docker å®¹å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        result = run_command(['docker', 'ps'], check=False)
        if config.DOCKER_CONTAINER in result.stdout:
            logger.info("âœ“ Docker å®¹å™¨è¿è¡Œæ­£å¸¸")
            return True
        else:
            logger.error(f"Docker å®¹å™¨ {config.DOCKER_CONTAINER} æœªè¿è¡Œ")
            return False
    except Exception as e:
        logger.error(f"æ£€æŸ¥ Docker å®¹å™¨å¤±è´¥: {e}")
        return False


def get_directory_size(path: Path) -> int:
    """è·å–ç›®å½•å¤§å°ï¼ˆKBï¼‰"""
    try:
        result = run_command(['du', '-sk', str(path)])
        return int(result.stdout.split()[0])
    except Exception as e:
        logger.warning(f"è·å–ç›®å½•å¤§å°å¤±è´¥ {path}: {e}")
        return 0


def get_commit_count(repo_path: Path) -> int:
    """è·å–ä»“åº“çš„æäº¤æ€»æ•°"""
    try:
        owner = repo_path.parent.name
        repo = repo_path.name
        container_path = f"/data/git/repositories/{owner}/{repo}"

        # åœ¨å®¹å™¨ä¸­ä½¿ç”¨ git ç”¨æˆ·æ‰§è¡Œ git rev-list --all --count
        result = run_command(
            [
                'docker',
                'exec',
                '-u',
                config.DOCKER_GIT_USER,
                config.DOCKER_CONTAINER,
                'git',
                '-C',
                container_path,
                'rev-list',
                '--all',
                '--count',
            ],
            check=False,
        )

        if result.returncode == 0:
            return int(result.stdout.strip())
        else:
            logger.warning(f"æ— æ³•è·å–æäº¤æ•° {repo_path}: {result.stderr}")
            return 0
    except Exception as e:
        logger.warning(f"è·å–æäº¤æ•°å¤±è´¥ {repo_path}: {e}")
        return 0


def is_mirror_repo(repo_path: Path) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯é•œåƒä»“åº“"""
    if not config.CHECK_MIRROR_ONLY:
        logger.info("    CHECK_MIRROR_ONLY=Falseï¼Œå¤‡ä»½æ‰€æœ‰ä»“åº“")
        return True  # ä¸æ£€æŸ¥ï¼Œå¤‡ä»½æ‰€æœ‰ä»“åº“

    try:
        owner = repo_path.parent.name
        repo = repo_path.name
        container_path = f"/data/git/repositories/{owner}/{repo}"

        logger.info(f"    æ£€æŸ¥å®¹å™¨è·¯å¾„: {container_path}")
        result = run_command(
            [
                'docker',
                'exec',
                '-u',
                config.DOCKER_GIT_USER,
                config.DOCKER_CONTAINER,
                'git',
                '-C',
                container_path,
                'config',
                '--get',
                'remote.origin.url',
            ],
            check=False,
        )

        if result.returncode == 0:
            logger.info(f"    âœ“ æ˜¯é•œåƒä»“åº“ï¼Œremote.origin.url: {result.stdout.strip()}")
            return True
        else:
            logger.info("    âœ— ä¸æ˜¯é•œåƒä»“åº“ï¼Œæœªæ‰¾åˆ° remote.origin.url")
            return False
    except Exception as e:
        logger.warning(f"    æ£€æŸ¥é•œåƒä»“åº“å¤±è´¥ {repo_path}: {e}")
        return False


# ============ å¤‡ä»½åŠŸèƒ½ ============
class RepositoryBackup:
    def __init__(self, repo_path: Path):
        self.repo_path = repo_path
        self.owner = repo_path.parent.name
        self.repo_name = repo_path.name.replace('.git', '')
        self.full_name = f"{self.owner}/{self.repo_name}"
        self.backup_dir = Path(config.BACKUP_ROOT) / self.owner / self.repo_name
        self.snapshot_dir = self.backup_dir / "snapshots"
        self.archive_dir = self.backup_dir / "archives"

    def should_backup(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤‡ä»½è¿™ä¸ªä»“åº“"""
        # æ£€æŸ¥ç»„ç»‡è¿‡æ»¤ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        if config.BACKUP_ORGANIZATIONS:
            logger.info(f"    ç»„ç»‡è¿‡æ»¤: {config.BACKUP_ORGANIZATIONS}")
            logger.info(f"    å½“å‰ç»„ç»‡: {self.owner}")

            # å°†ç»„ç»‡åè½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
            owner_lower = self.owner.lower()
            orgs_lower = [org.lower() for org in config.BACKUP_ORGANIZATIONS]

            if owner_lower not in orgs_lower:
                logger.info(f"    âŒ è·³è¿‡ {self.full_name}: ä¸åœ¨å¤‡ä»½ç»„ç»‡åˆ—è¡¨ä¸­")
                return False
            logger.info("    âœ“ ç»„ç»‡åŒ¹é…")

        # æ£€æŸ¥æ˜¯å¦æ˜¯é•œåƒä»“åº“
        logger.info(f"    æ£€æŸ¥é•œåƒä»“åº“: CHECK_MIRROR_ONLY={config.CHECK_MIRROR_ONLY}")
        if not is_mirror_repo(self.repo_path):
            logger.info(f"    âŒ è·³è¿‡ {self.full_name}: ä¸æ˜¯é•œåƒä»“åº“")
            return False

        logger.info(f"    âœ“ å°†å¤‡ä»½ {self.full_name}")
        return True

    def create_snapshot(self) -> Optional[Path]:
        """åˆ›å»ºå¿«ç…§ï¼Œè¿”å›å¿«ç…§è·¯å¾„"""
        try:
            date_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            snapshot_path = self.snapshot_dir / date_stamp

            # åˆ›å»ºå¿«ç…§ç›®å½•
            self.snapshot_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"  åˆ›å»ºå¿«ç…§ç›®å½•: {self.snapshot_dir}")

            logger.info(f"  åˆ›å»ºå¿«ç…§: {self.full_name}")

            # å°è¯•ä½¿ç”¨ç¡¬é“¾æ¥åˆ›å»ºå¿«ç…§ (cp -al)ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ™®é€šå¤åˆ¶
            result = run_command(
                ['cp', '-al', str(self.repo_path), str(snapshot_path)], check=False
            )

            if result.returncode != 0:
                # ç¡¬é“¾æ¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è·¨æ–‡ä»¶ç³»ç»Ÿï¼‰ï¼Œä½¿ç”¨æ™®é€šå¤åˆ¶
                if (
                    "Invalid cross-device link" in result.stderr
                    or "cross-device" in result.stderr.lower()
                ):
                    logger.warning("  âš ï¸  æ— æ³•ä½¿ç”¨ç¡¬é“¾æ¥ï¼ˆè·¨æ–‡ä»¶ç³»ç»Ÿï¼‰ï¼Œä½¿ç”¨æ™®é€šå¤åˆ¶...")
                    result = run_command(
                        ['cp', '-a', str(self.repo_path), str(snapshot_path)],
                        check=False,
                    )

                if result.returncode != 0:
                    logger.error(f"  âœ— å¿«ç…§å¤±è´¥: {self.full_name}")
                    logger.error(f"  é”™è¯¯: {result.stderr}")
                    return None

            # è·å–å½“å‰æäº¤æ•°
            current_commits = get_commit_count(self.repo_path)

            # è®°å½•å…ƒæ•°æ®
            meta_file = snapshot_path / ".snapshot_meta"
            with open(meta_file, 'w') as f:
                f.write(f"timestamp={datetime.now().isoformat()}\n")
                f.write(f"source={self.repo_path}\n")
                f.write(f"repo_name={self.full_name}\n")
                f.write(f"commit_count={current_commits}\n")

            logger.info(f"  âœ“ å¿«ç…§æˆåŠŸ: {date_stamp} (æäº¤æ•°: {current_commits})")
            return snapshot_path

        except Exception as e:
            logger.error(f"  âœ— åˆ›å»ºå¿«ç…§å¤±è´¥ {self.full_name}: {e}")
            return None

    def check_commit_changes(
        self, snapshot_path: Optional[Path] = None
    ) -> Optional[int]:
        """æ£€æµ‹æäº¤æ•°å˜åŒ–ï¼Œè¿”å›å‡å°‘ç™¾åˆ†æ¯”ã€‚å¦‚æœæ£€æµ‹åˆ°å¼‚å¸¸ï¼Œä¿æŠ¤ä¸Šä¸€æ¬¡çš„å¿«ç…§ï¼ˆæ­£å¸¸çŠ¶æ€ï¼‰"""
        commit_tracking_file = self.backup_dir / ".commit_tracking"
        size_tracking_file = self.backup_dir / ".size_tracking"

        # è·å–å½“å‰æäº¤æ•°å’Œå¤§å°
        current_commits = get_commit_count(self.repo_path)
        current_size = get_directory_size(self.repo_path)

        # é¦–æ¬¡å¤‡ä»½
        if not commit_tracking_file.exists():
            commit_tracking_file.parent.mkdir(parents=True, exist_ok=True)
            commit_tracking_file.write_text(str(current_commits))
            size_tracking_file.write_text(str(current_size))
            logger.info(f"  åˆå§‹æäº¤æ•°: {current_commits}, å¤§å°: {current_size}KB")
            return None

        # è¯»å–ä¸Šæ¬¡çš„æäº¤æ•°å’Œå¤§å°
        try:
            prev_commits = int(commit_tracking_file.read_text().strip())
            prev_size = (
                int(size_tracking_file.read_text().strip())
                if size_tracking_file.exists()
                else 0
            )
        except Exception as e:
            logger.warning(f"è¯»å–è·Ÿè¸ªæ–‡ä»¶å¤±è´¥: {e}")
            commit_tracking_file.write_text(str(current_commits))
            size_tracking_file.write_text(str(current_size))
            return None

        # æ£€æŸ¥æäº¤æ•°æ˜¯å¦æ˜¾è‘—å‡å°‘
        alert_triggered = False
        alert_messages = []

        if current_commits < prev_commits:
            decrease_percent = ((prev_commits - current_commits) * 100) // prev_commits

            if decrease_percent > config.COMMIT_DECREASE_THRESHOLD:
                alert_triggered = True
                alert_messages.append(f"æäº¤æ•°å¼‚å¸¸å‡å°‘: {decrease_percent}%")
                alert_messages.append(
                    f"ä¸Šæ¬¡: {prev_commits} commits â†’ å½“å‰: {current_commits} commits"
                )
                logger.warning(
                    f"  âš ï¸  æäº¤æ•°å‡å°‘ {decrease_percent}% (ä» {prev_commits} åˆ° {current_commits})"
                )

        # åŒæ—¶æ£€æŸ¥å¤§å°å˜åŒ–ï¼ˆè¾…åŠ©å‚è€ƒï¼‰
        if current_size < prev_size:
            size_decrease = ((prev_size - current_size) * 100) // prev_size
            if size_decrease > config.SIZE_DECREASE_THRESHOLD:
                if not alert_triggered:
                    alert_messages.append(f"ä»“åº“å¤§å°å¼‚å¸¸å‡å°‘: {size_decrease}%")
                else:
                    alert_messages.append(f"åŒæ—¶ä»“åº“å¤§å°å‡å°‘: {size_decrease}%")
                alert_messages.append(f"ä¸Šæ¬¡: {prev_size}KB â†’ å½“å‰: {current_size}KB")
                logger.warning(f"  âš ï¸  å¤§å°å‡å°‘ {size_decrease}%")
                alert_triggered = True

        # å¦‚æœè§¦å‘å‘Šè­¦ï¼Œè®°å½•åˆ°æ–‡ä»¶
        if alert_triggered:
            alert_file = self.backup_dir / ".alerts"
            with open(alert_file, 'a') as f:
                f.write(f"\n[{datetime.now().isoformat()}]\n")
                for msg in alert_messages:
                    f.write(f"{msg}\n")
                f.write("å¯èƒ½åŸå› : force pushã€åˆ†æ”¯åˆ é™¤æˆ–å†å²é‡å†™\n")

            # æ·»åŠ åˆ°å®¡æ ¸åˆ—è¡¨
            need_review_file = Path(config.BACKUP_ROOT) / ".need_review"
            with open(need_review_file, 'a') as f:
                f.write(f"{self.full_name}\n")

            # ä¿æŠ¤ä¸Šä¸€æ¬¡çš„å¿«ç…§ï¼ˆå¼‚å¸¸å‘ç”Ÿå‰çš„æ­£å¸¸çŠ¶æ€ï¼‰
            if config.PROTECT_ABNORMAL_SNAPSHOTS:
                previous_snapshot = self.get_previous_snapshot(snapshot_path)
                if previous_snapshot:
                    self.protect_snapshot(previous_snapshot, alert_messages)
                else:
                    logger.warning("  âš ï¸  æœªæ‰¾åˆ°ä¸Šä¸€æ¬¡å¿«ç…§ï¼Œæ— æ³•è‡ªåŠ¨ä¿æŠ¤")

            # æ›´æ–°è·Ÿè¸ªè®°å½•
            commit_tracking_file.write_text(str(current_commits))
            size_tracking_file.write_text(str(current_size))
            return decrease_percent if current_commits < prev_commits else size_decrease

        # æ›´æ–°è·Ÿè¸ªè®°å½•
        commit_tracking_file.write_text(str(current_commits))
        size_tracking_file.write_text(str(current_size))
        return None

    def get_previous_snapshot(self, current_snapshot: Optional[Path]) -> Optional[Path]:
        """è·å–ä¸Šä¸€æ¬¡çš„å¿«ç…§ï¼ˆå½“å‰å¿«ç…§ä¹‹å‰çš„æœ€è¿‘å¿«ç…§ï¼‰"""
        if not self.snapshot_dir.exists():
            return None

        try:
            # è·å–æ‰€æœ‰å¿«ç…§ï¼ŒæŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            snapshots = sorted(
                [s for s in self.snapshot_dir.iterdir() if s.is_dir()],
                key=lambda x: x.stat().st_mtime,
                reverse=True,
            )

            if not snapshots:
                return None

            # å¦‚æœæä¾›äº†å½“å‰å¿«ç…§ï¼Œæ‰¾åˆ°å®ƒä¹‹å‰çš„å¿«ç…§
            if current_snapshot:
                for i, snapshot in enumerate(snapshots):
                    if snapshot == current_snapshot:
                        # è¿”å›ä¸‹ä¸€ä¸ªï¼ˆæ›´æ—©çš„ï¼‰å¿«ç…§
                        if i + 1 < len(snapshots):
                            logger.info(f"  æ‰¾åˆ°ä¸Šä¸€æ¬¡å¿«ç…§: {snapshots[i + 1].name}")
                            return snapshots[i + 1]
                        break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æˆ–æ²¡æä¾›å½“å‰å¿«ç…§ï¼Œè¿”å›æœ€æ–°çš„ï¼ˆç¬¬ä¸€ä¸ªï¼‰
            if snapshots:
                logger.info(f"  æ‰¾åˆ°ä¸Šä¸€æ¬¡å¿«ç…§: {snapshots[0].name}")
                return snapshots[0]
            return None

        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾ä¸Šä¸€æ¬¡å¿«ç…§å¤±è´¥: {e}")
            return None

    def protect_snapshot(self, snapshot_path: Path, reasons: List[str]):
        """æ ‡è®°å¿«ç…§ä¸ºæ°¸ä¹…ä¿ç•™"""
        try:
            protect_file = snapshot_path / ".protected"
            with open(protect_file, 'w') as f:
                f.write("# æ­¤å¿«ç…§å·²è¢«æ ‡è®°ä¸ºæ°¸ä¹…ä¿ç•™\n")
                f.write("# ä¿æŠ¤åŸå› : å¼‚å¸¸å‘ç”Ÿå‰çš„æ­£å¸¸çŠ¶æ€\n")
                f.write(f"# æ ‡è®°æ—¶é—´: {datetime.now().isoformat()}\n")
                f.write(f"# ä»“åº“: {self.full_name}\n")
                f.write("#\n")
                f.write("# æ£€æµ‹åˆ°çš„å¼‚å¸¸ï¼ˆå‘ç”Ÿåœ¨æ­¤å¿«ç…§ä¹‹åï¼‰:\n")
                for reason in reasons:
                    f.write(f"#   - {reason}\n")
                f.write("#\n")
                f.write("# æ­¤å¿«ç…§ä¿å­˜çš„æ˜¯å¼‚å¸¸å‘ç”Ÿå‰çš„æ­£å¸¸çŠ¶æ€ï¼Œå¯å®‰å…¨æ¢å¤\n")
                f.write("# å¦‚éœ€å–æ¶ˆä¿æŠ¤ï¼Œåˆ é™¤æ­¤æ–‡ä»¶å³å¯\n")
            logger.info(
                f"  ğŸ”’ å¿«ç…§å·²æ ‡è®°ä¸ºæ°¸ä¹…ä¿ç•™: {snapshot_path.name} ï¼ˆå¼‚å¸¸å‰çš„æ­£å¸¸çŠ¶æ€ï¼‰"
            )
        except Exception as e:
            logger.warning(f"æ ‡è®°å¿«ç…§ä¿æŠ¤å¤±è´¥: {e}")

    def cleanup_old_snapshots(self):
        """æ¸…ç†æ—§å¿«ç…§ï¼ˆè·³è¿‡è¢«ä¿æŠ¤çš„å¿«ç…§ï¼‰"""
        if not self.snapshot_dir.exists():
            return

        cutoff_date = datetime.now() - timedelta(days=config.SNAPSHOT_RETENTION_DAYS)
        deleted_count = 0
        protected_count = 0

        for snapshot in self.snapshot_dir.iterdir():
            if not snapshot.is_dir():
                continue

            # æ£€æŸ¥æ˜¯å¦è¢«ä¿æŠ¤
            protect_file = snapshot / ".protected"
            if protect_file.exists():
                protected_count += 1
                continue  # è·³è¿‡è¢«ä¿æŠ¤çš„å¿«ç…§

            # æ£€æŸ¥ä¿®æ”¹æ—¶é—´
            mtime = datetime.fromtimestamp(snapshot.stat().st_mtime)
            if mtime < cutoff_date:
                try:
                    shutil.rmtree(snapshot)
                    deleted_count += 1
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ—§å¿«ç…§å¤±è´¥ {snapshot}: {e}")

        if deleted_count > 0:
            logger.info(f"  æ¸…ç†æ—§å¿«ç…§: {deleted_count} ä¸ª")
        if protected_count > 0:
            logger.info(f"  è·³è¿‡å—ä¿æŠ¤å¿«ç…§: {protected_count} ä¸ª")

    def create_monthly_archive(self):
        """åˆ›å»ºæœˆåº¦å½’æ¡£"""
        month_stamp = datetime.now().strftime('%Y%m')
        archive_file = self.archive_dir / f"archive-{month_stamp}.bundle"

        # æ£€æŸ¥æœ¬æœˆæ˜¯å¦å·²åˆ›å»º
        if archive_file.exists():
            return

        self.archive_dir.mkdir(parents=True, exist_ok=True)
        logger.info("  åˆ›å»ºæœˆåº¦å½’æ¡£...")

        try:
            # åœ¨å®¹å™¨ä¸­åˆ›å»º bundle
            container_repo_path = (
                f"/data/git/repositories/{self.owner}/{self.repo_name}.git"
            )

            # åˆ›å»º bundle
            run_command(
                [
                    'docker',
                    'exec',
                    '-u',
                    config.DOCKER_GIT_USER,
                    config.DOCKER_CONTAINER,
                    'git',
                    '-C',
                    container_repo_path,
                    'bundle',
                    'create',
                    '/tmp/temp.bundle',
                    '--all',
                ]
            )

            # å¤åˆ¶åˆ°å®¿ä¸»æœº
            run_command(
                [
                    'docker',
                    'cp',
                    f"{config.DOCKER_CONTAINER}:/tmp/temp.bundle",
                    str(archive_file),
                ]
            )

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            run_command(
                [
                    'docker',
                    'exec',
                    '-u',
                    config.DOCKER_GIT_USER,
                    config.DOCKER_CONTAINER,
                    'rm',
                    '/tmp/temp.bundle',
                ]
            )

            logger.info("  âœ“ å½’æ¡£æˆåŠŸ")

            # æ¸…ç†æ—§å½’æ¡£
            cutoff_date = datetime.now() - timedelta(
                days=config.ARCHIVE_RETENTION_MONTHS * 30
            )
            for archive in self.archive_dir.glob("*.bundle"):
                mtime = datetime.fromtimestamp(archive.stat().st_mtime)
                if mtime < cutoff_date:
                    archive.unlink()

        except Exception as e:
            logger.error(f"  âœ— åˆ›å»ºå½’æ¡£å¤±è´¥: {e}")

    def process(self):
        """å¤„ç†å•ä¸ªä»“åº“çš„å®Œæ•´å¤‡ä»½æµç¨‹"""
        logger.info("=" * 50)
        logger.info(f"å¤„ç†ä»“åº“: {self.full_name}")
        logger.info(f"ä»“åº“è·¯å¾„: {self.repo_path}")

        # 1. åˆ›å»ºå¿«ç…§
        snapshot_path = self.create_snapshot()
        if not snapshot_path:
            logger.error("å¿«ç…§åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡åç»­æ“ä½œ")
            return

        # 2. æ£€æµ‹æäº¤æ•°å’Œå¤§å°å˜åŒ–ï¼ˆå¦‚æœå¼‚å¸¸ä¼šè‡ªåŠ¨æ ‡è®°å¿«ç…§ä¸ºæ°¸ä¹…ä¿ç•™ï¼‰
        self.check_commit_changes(snapshot_path)

        # 3. æ¸…ç†æ—§å¿«ç…§ï¼ˆè·³è¿‡è¢«ä¿æŠ¤çš„ï¼‰
        self.cleanup_old_snapshots()

        # 4. æ¯æœˆ1å·åˆ›å»ºå½’æ¡£
        if datetime.now().day == 1:
            self.create_monthly_archive()

        # 5. ç”Ÿæˆæ¢å¤è„šæœ¬
        self.generate_restore_script()

    def generate_restore_script(self):
        """ç”Ÿæˆæ¢å¤è„šæœ¬"""
        restore_script = self.backup_dir / "restore.sh"

        script_content = f"""#!/bin/bash

REPO_NAME="{self.full_name}"
SNAPSHOT_DIR="{self.snapshot_dir}"
CONTAINER="{config.DOCKER_CONTAINER}"
GIT_USER="{config.DOCKER_GIT_USER}"
CONTAINER_REPO_PATH="/data/git/repositories/{self.owner}/{self.repo_name}.git"
HOST_REPO_PATH="{self.repo_path}"

echo "=========================================="
echo "Gitea é•œåƒä»“åº“æ¢å¤å·¥å…·"
echo "=========================================="
echo "ä»“åº“: $REPO_NAME"
echo ""

# åˆ—å‡ºå¯ç”¨å¿«ç…§
echo "å¯ç”¨çš„å¿«ç…§:"
mapfile -t snapshots < <(ls -td "$SNAPSHOT_DIR"/* 2>/dev/null)
if [ ${{#snapshots[@]}} -eq 0 ]; then
    echo "é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°å¿«ç…§"
    exit 1
fi

# æ˜¾ç¤ºå¿«ç…§ï¼ˆç´¢å¼•ä»1å¼€å§‹ï¼‰
for i in "${{!snapshots[@]}}"; do
    snapshot_name=$(basename "${{snapshots[$i]}}")
    display_num=$((i + 1))
    echo "  [$display_num] $snapshot_name"

    # æ˜¾ç¤ºå¿«ç…§ä¿¡æ¯
    if [ -f "${{snapshots[$i]}}/.snapshot_meta" ]; then
        grep timestamp "${{snapshots[$i]}}/.snapshot_meta" | sed 's/^/         /'
    fi
done

echo ""
echo "æ¢å¤é€‰é¡¹:"
echo "  1) æ¢å¤åˆ°åŸä»“åº“ä½ç½®ï¼ˆä¼šè¦†ç›–ç°æœ‰ä»“åº“ï¼‰"
echo "  2) å¯¼å‡ºä¸ºæ–°ä»“åº“ï¼ˆä¸å½±å“åŸä»“åº“ï¼‰"
echo "  3) å¯¼å‡ºä¸º Git Bundle æ–‡ä»¶"
echo ""
read -p "é€‰æ‹©æ¢å¤æ–¹å¼ [1]: " restore_mode
restore_mode=${{restore_mode:-1}}

echo ""
read -p "é€‰æ‹©è¦æ¢å¤çš„å¿«ç…§ç¼–å· [1]: " choice
choice=${{choice:-1}}

# è½¬æ¢ä¸ºæ•°ç»„ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
array_index=$((choice - 1))

if [ $array_index -lt 0 ] || [ -z "${{snapshots[$array_index]}}" ]; then
    echo "é”™è¯¯: æ— æ•ˆçš„é€‰æ‹©"
    exit 1
fi

SELECTED_SNAPSHOT="${{snapshots[$array_index]}}"
echo ""
echo "å·²é€‰æ‹©å¿«ç…§: $(basename $SELECTED_SNAPSHOT)"
echo ""

case $restore_mode in
    1)
        # æ¢å¤åˆ°åŸä½ç½®
        echo "âš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†è¦†ç›–å®¹å™¨ä¸­çš„åŸä»“åº“"
        echo "âš ï¸  æ³¨æ„: å¦‚æœè¿™æ˜¯é•œåƒä»“åº“ï¼Œä¸‹æ¬¡åŒæ­¥æ—¶å¯èƒ½å†æ¬¡è¢«æºä»“åº“è¦†ç›–"
        read -p "ç¡®è®¤ç»§ç»­? (yes/NO): " confirm

        if [ "$confirm" != "yes" ]; then
            echo "å·²å–æ¶ˆ"
            exit 0
        fi

        echo ""
        echo "æ­£åœ¨æ¢å¤åˆ°åŸä½ç½®..."

        # åœæ­¢å®¹å™¨
        echo "1. åœæ­¢ Docker å®¹å™¨..."
        docker stop $CONTAINER

        # å¤‡ä»½å½“å‰ä»“åº“
        BACKUP_CURRENT="${{HOST_REPO_PATH}}.backup-$(date +%Y%m%d-%H%M%S)"
        echo "2. å¤‡ä»½å½“å‰ä»“åº“åˆ°: $BACKUP_CURRENT"
        mv "$HOST_REPO_PATH" "$BACKUP_CURRENT"

        # æ¢å¤å¿«ç…§
        echo "3. æ¢å¤å¿«ç…§..."
        cp -a "$SELECTED_SNAPSHOT" "$HOST_REPO_PATH"

        # ä¿®å¤æƒé™
        echo "4. ä¿®å¤æ–‡ä»¶æƒé™..."
        docker exec $CONTAINER chown -R git:git "$CONTAINER_REPO_PATH"

        # å¯åŠ¨å®¹å™¨
        echo "5. å¯åŠ¨ Docker å®¹å™¨..."
        docker start $CONTAINER

        # ç­‰å¾…å®¹å™¨å¯åŠ¨
        sleep 2

        # æ›´æ–° server infoï¼ˆä¿®å¤ git hooksï¼‰
        echo "6. æ›´æ–°ä»“åº“ä¿¡æ¯..."
        docker exec -u $GIT_USER $CONTAINER git -C "$CONTAINER_REPO_PATH" update-server-info

        echo ""
        echo "âœ“ æ¢å¤å®Œæˆ!"
        echo ""
        echo "å¦‚éœ€å›æ»šï¼Œå½“å‰ä»“åº“å·²å¤‡ä»½è‡³:"
        echo "  $BACKUP_CURRENT"
        echo ""
        echo "éªŒè¯å‘½ä»¤:"
        echo "  docker exec -u $GIT_USER $CONTAINER git -C $CONTAINER_REPO_PATH log --oneline -5"
        ;;

    2)
        # å¯¼å‡ºä¸ºæ–°ä»“åº“
        echo "å¯¼å‡ºä¸ºæ–°ä»“åº“ï¼ˆç‹¬ç«‹å‰¯æœ¬ï¼Œä¸å½±å“åŸä»“åº“ï¼‰"
        read -p "è¾“å…¥æ–°ä»“åº“åç§°ï¼ˆå¦‚ test-restoredï¼‰: " new_repo_name

        if [ -z "$new_repo_name" ]; then
            echo "é”™è¯¯: ä»“åº“åç§°ä¸èƒ½ä¸ºç©º"
            exit 1
        fi

        # å¯¼å‡ºè·¯å¾„
        EXPORT_PATH="${{HOST_REPO_PATH%/*}}/${{new_repo_name}}.git"

        if [ -d "$EXPORT_PATH" ]; then
            echo "é”™è¯¯: ä»“åº“å·²å­˜åœ¨: $EXPORT_PATH"
            exit 1
        fi

        echo ""
        echo "æ­£åœ¨å¯¼å‡ºæ–°ä»“åº“..."

        # å¤åˆ¶å¿«ç…§
        echo "1. å¤åˆ¶ä»“åº“æ•°æ®..."
        cp -a "$SELECTED_SNAPSHOT" "$EXPORT_PATH"

        # ä¿®å¤æ–‡ä»¶æƒé™
        echo "2. ä¿®å¤æ–‡ä»¶æƒé™..."
        # åœ¨å®¿ä¸»æœºä¸Šä¿®å¤æƒé™ï¼ˆè·å– git ç”¨æˆ·çš„ UID/GIDï¼‰
        GIT_UID=$(docker exec $CONTAINER id -u git 2>/dev/null || echo "1000")
        GIT_GID=$(docker exec $CONTAINER id -g git 2>/dev/null || echo "1000")
        echo "   è®¾ç½®æ‰€æœ‰è€…ä¸º $GIT_UID:$GIT_GID"
        chown -R $GIT_UID:$GIT_GID "$EXPORT_PATH"

        # æ›´æ–°é…ç½®ï¼ˆç§»é™¤é•œåƒé…ç½®ï¼‰
        NEW_CONTAINER_PATH="/data/git/repositories/$(basename $(dirname $EXPORT_PATH))/${{new_repo_name}}.git"
        echo "3. ç§»é™¤é•œåƒé…ç½®..."
        docker exec -u $GIT_USER $CONTAINER git -C "$NEW_CONTAINER_PATH" config --unset remote.origin.url 2>/dev/null || true
        docker exec -u $GIT_USER $CONTAINER git -C "$NEW_CONTAINER_PATH" config --unset remote.origin.fetch 2>/dev/null || true
        docker exec -u $GIT_USER $CONTAINER git -C "$NEW_CONTAINER_PATH" update-server-info

        echo ""
        echo "âœ“ ä»“åº“å¯¼å‡ºå®Œæˆ!"
        echo ""
        echo "æ–°ä»“åº“ä½ç½®: $EXPORT_PATH"
        echo ""

        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "âœ“ ä»“åº“æ–‡ä»¶å·²å¯¼å‡ºå®Œæˆ!"
        echo ""
        # è·å–å®é™…çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ï¼ˆå°å†™ï¼‰
        OWNER_NAME=$(basename $(dirname $EXPORT_PATH))
        SEARCH_PATH="$OWNER_NAME/$new_repo_name"

        echo "æ–°ä»“åº“ä½ç½®: $EXPORT_PATH"
        echo ""
        echo "ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šåœ¨ Gitea ä¸­é‡‡é›†ä»“åº“"
        echo ""
        echo "ç”±äº Gitea æ²¡æœ‰å‘½ä»¤è¡Œé‡‡é›†åŠŸèƒ½ï¼Œéœ€è¦æ‰‹åŠ¨æ“ä½œï¼š"
        echo ""
        echo "1. ç™»å½• Gitea ç®¡ç†å‘˜è´¦å·"
        echo ""
        echo "2. è¿›å…¥ç®¡ç†åå°ï¼š"
        echo "   è®¿é—®: http://your-gitea/-/admin/repos/unadopted"
        echo "   æˆ–ç‚¹å‡»: å³ä¸Šè§’å¤´åƒ -> ç®¡ç†åå° -> ä»“åº“ç®¡ç† -> æœªé‡‡é›†çš„Gitä»“åº“"
        echo ""
        echo "3. æœç´¢ä»“åº“ï¼ˆé‡è¦ï¼åŒºåˆ†å¤§å°å†™ï¼‰ï¼š"
        echo "   åœ¨æœç´¢æ¡†è¾“å…¥: $SEARCH_PATH"
        echo "   âš ï¸  æ³¨æ„: å¿…é¡»ä½¿ç”¨å®é™…æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ï¼ˆå°å†™ï¼‰ï¼Œå¤§å°å†™æ•æ„Ÿ"
        echo ""
        echo "4. æ‰¾åˆ°ä»“åº“åï¼Œç‚¹å‡»å³ä¾§çš„ã€Œé‡‡é›†ã€æŒ‰é’®"
        echo ""
        echo "5. å®Œæˆï¼è®¿é—®: http://your-gitea/$SEARCH_PATH"
        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        ;;

    3)
        # å¯¼å‡ºä¸º Bundle
        read -p "è¾“å…¥ Bundle æ–‡ä»¶ä¿å­˜è·¯å¾„ [/tmp/${{REPO_NAME//\\//-}}.bundle]: " bundle_path
        bundle_path=${{bundle_path:-/tmp/${{REPO_NAME//\\//-}}.bundle}}

        echo ""
        echo "æ­£åœ¨å¯¼å‡º Git Bundle..."

        # ä¸´æ—¶æŒ‚è½½å¿«ç…§åˆ°å®¹å™¨
        TMP_MOUNT="/tmp/restore-${{RANDOM}}"
        mkdir -p "$TMP_MOUNT"
        cp -a "$SELECTED_SNAPSHOT" "$TMP_MOUNT/repo.git"

        # åˆ›å»º bundle
        docker exec -u $GIT_USER $CONTAINER sh -c "cd /tmp && git clone --bare $TMP_MOUNT/repo.git temp-repo.git && cd temp-repo.git && git bundle create /tmp/export.bundle --all"

        # å¤åˆ¶ bundle åˆ°å®¿ä¸»æœº
        docker cp "$CONTAINER:/tmp/export.bundle" "$bundle_path"

        # æ¸…ç†
        docker exec $CONTAINER rm -rf /tmp/temp-repo.git /tmp/export.bundle
        rm -rf "$TMP_MOUNT"

        echo ""
        echo "âœ“ å¯¼å‡ºå®Œæˆ!"
        echo ""
        echo "Bundle æ–‡ä»¶: $bundle_path"
        echo ""
        echo "ä½¿ç”¨æ–¹æ³•:"
        echo "  git clone $bundle_path restored-repo"
        ;;

    *)
        echo "é”™è¯¯: æ— æ•ˆçš„æ¢å¤æ–¹å¼"
        exit 1
        ;;
esac
"""

        restore_script.write_text(script_content)
        restore_script.chmod(0o755)


# ============ æŠ¥å‘Šç”Ÿæˆ ============
def send_backup_notification(processed_count: int, skipped_count: int):
    """å‘é€å¤‡ä»½é€šçŸ¥"""
    if not notifier:
        return

    backup_root = Path(config.BACKUP_ROOT)
    need_review_file = backup_root / ".need_review"
    has_alerts = need_review_file.exists() and need_review_file.stat().st_size > 0

    # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    total_repos = 0
    total_commits = 0
    total_snapshots = 0
    alert_repos = []

    for org_dir in backup_root.iterdir():
        if not org_dir.is_dir() or org_dir.name.startswith('.'):
            continue
        for repo_dir in org_dir.iterdir():
            if not repo_dir.is_dir():
                continue
            total_repos += 1

            # ç»Ÿè®¡å¿«ç…§
            snapshot_dir = repo_dir / "snapshots"
            if snapshot_dir.exists():
                total_snapshots += len(
                    [s for s in snapshot_dir.iterdir() if s.is_dir()]
                )

            # ç»Ÿè®¡æäº¤æ•°
            commit_file = repo_dir / ".commit_tracking"
            if commit_file.exists():
                try:
                    commits = int(commit_file.read_text().strip())
                    total_commits += commits
                except Exception:
                    pass

            # æ£€æŸ¥å¼‚å¸¸
            alert_file = repo_dir / ".alerts"
            if alert_file.exists():
                repo_name = f"{org_dir.name}/{repo_dir.name}"
                alert_repos.append(repo_name)

    # æ„å»ºæŠ¥å‘Šæ•°æ®
    report_data = {
        'total_repos': total_repos,
        'total_commits': total_commits,
        'total_snapshots': total_snapshots,
        'processed_count': processed_count,
        'skipped_count': skipped_count,
        'has_alerts': has_alerts,
        'alert_repos': alert_repos,
        'total_size_mb': 0,  # å¯ä»¥æ·»åŠ å¤§å°ç»Ÿè®¡
    }

    # å‘é€é€šçŸ¥
    notifier.send_backup_report(report_data)
    logger.info("å¤‡ä»½é€šçŸ¥å·²å‘é€")


def cleanup_old_reports():
    """æ¸…ç†æ—§æŠ¥å‘Šï¼ˆè·³è¿‡è¢«ä¿æŠ¤çš„æŠ¥å‘Šï¼‰"""
    report_dir = Path(config.REPORT_DIR)
    if not report_dir.exists():
        return

    cutoff_date = datetime.now() - timedelta(days=config.REPORT_RETENTION_DAYS)
    deleted_count = 0
    protected_count = 0

    for report_file in report_dir.glob("report-*.md"):
        try:
            # æ£€æŸ¥æ˜¯å¦è¢«ä¿æŠ¤
            protect_file = report_file.with_suffix('.md.protected')
            if protect_file.exists():
                protected_count += 1
                continue  # è·³è¿‡è¢«ä¿æŠ¤çš„æŠ¥å‘Š

            mtime = datetime.fromtimestamp(report_file.stat().st_mtime)
            if mtime < cutoff_date:
                report_file.unlink()
                deleted_count += 1
        except Exception as e:
            logger.warning(f"åˆ é™¤æ—§æŠ¥å‘Šå¤±è´¥ {report_file}: {e}")

    if deleted_count > 0:
        logger.info(f"æ¸…ç†äº† {deleted_count} ä¸ªæ—§æŠ¥å‘Š")
    if protected_count > 0:
        logger.info(f"è·³è¿‡å—ä¿æŠ¤æŠ¥å‘Š: {protected_count} ä¸ª")


def generate_report():
    """ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š"""
    logger.info("ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š...")

    backup_root = Path(config.BACKUP_ROOT)
    report_dir = Path(config.REPORT_DIR)
    report_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æŠ¥å‘Šæ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    report_file = report_dir / f"report-{timestamp}.md"
    latest_report = Path(config.LATEST_REPORT)

    # ç»Ÿè®¡ä¿¡æ¯
    total_repos = 0
    total_snapshots = 0
    total_archives = 0
    total_size = 0
    total_commits = 0
    repo_details = []

    # éå†æ‰€æœ‰ä»“åº“
    for org_dir in backup_root.iterdir():
        if not org_dir.is_dir() or org_dir.name.startswith('.'):
            continue

        for repo_dir in org_dir.iterdir():
            if not repo_dir.is_dir():
                continue

            repo_name = f"{org_dir.name}/{repo_dir.name}"
            total_repos += 1

            # ç»Ÿè®¡å¿«ç…§
            snapshot_count = 0
            protected_snapshot_count = 0
            latest_snapshot = "æ— "
            snapshot_dir = repo_dir / "snapshots"
            if snapshot_dir.exists():
                snapshots = sorted(
                    snapshot_dir.iterdir(),
                    key=lambda x: x.stat().st_mtime,
                    reverse=True,
                )
                all_snapshots = [s for s in snapshots if s.is_dir()]
                snapshot_count = len(all_snapshots)
                # ç»Ÿè®¡å—ä¿æŠ¤çš„å¿«ç…§
                protected_snapshot_count = len(
                    [s for s in all_snapshots if (s / ".protected").exists()]
                )
                if snapshots:
                    latest_snapshot = snapshots[0].name
                total_snapshots += snapshot_count

            # ç»Ÿè®¡å½’æ¡£
            archive_count = 0
            archive_dir = repo_dir / "archives"
            if archive_dir.exists():
                archive_count = len(list(archive_dir.glob("*.bundle")))
                total_archives += archive_count

            # è®¡ç®—å¤§å°
            dir_size = get_directory_size(repo_dir)
            total_size += dir_size

            # è¯»å–æäº¤æ•°å’Œå†å²å˜åŒ–
            commit_count = "N/A"
            commit_change = None
            commit_tracking_file = repo_dir / ".commit_tracking"
            if commit_tracking_file.exists():
                try:
                    commit_count = commit_tracking_file.read_text().strip()
                    if commit_count.isdigit():
                        total_commits += int(commit_count)
                except Exception:
                    pass

            # æ£€æŸ¥æ˜¯å¦æœ‰æäº¤æ•°å˜åŒ–å‘Šè­¦
            alert_file = repo_dir / ".alerts"
            if alert_file.exists():
                try:
                    alert_content = alert_file.read_text()
                    # æŸ¥æ‰¾æœ€åä¸€æ¬¡æäº¤æ•°å˜åŒ–è®°å½•
                    if "æäº¤æ•°å¼‚å¸¸å‡å°‘" in alert_content:
                        lines = alert_content.strip().split('\n')
                        for line in lines:
                            if "æäº¤æ•°å¼‚å¸¸å‡å°‘" in line:
                                commit_change = line
                                break
                except Exception:
                    pass

            repo_details.append(
                {
                    'name': repo_name,
                    'snapshot_count': snapshot_count,
                    'protected_snapshots': protected_snapshot_count,
                    'latest_snapshot': latest_snapshot,
                    'archive_count': archive_count,
                    'size_kb': dir_size,
                    'commits': commit_count,
                    'commit_change': commit_change,
                }
            )

    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
    need_review_file = backup_root / ".need_review"
    has_alerts = need_review_file.exists() and need_review_file.stat().st_size > 0

    # ç”ŸæˆæŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Gitea é•œåƒä»“åº“å¤‡ä»½æŠ¥å‘Š\n\n")

        # å¦‚æœæœ‰å¼‚å¸¸ï¼Œåœ¨æ ‡é¢˜ä¸‹æ–¹æ ‡æ³¨
        if has_alerts:
            f.write("> ğŸ”’ **æ­¤æŠ¥å‘Šå·²è‡ªåŠ¨æ ‡è®°ä¸ºæ°¸ä¹…ä¿ç•™**ï¼ˆæ£€æµ‹åˆ°ä»“åº“å¼‚å¸¸ï¼‰\n\n")

        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {current_time}\n")
        f.write(f"**æŠ¥å‘Šæ–‡ä»¶**: {report_file.name}\n\n")

        # æ€»ä½“ç»Ÿè®¡
        f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- **å¤‡ä»½ä»“åº“æ•°**: {total_repos}\n")
        f.write(f"- **æ€»æäº¤æ•°**: {total_commits:,} commits\n")
        f.write(f"- **å¿«ç…§æ€»æ•°**: {total_snapshots}\n")
        f.write(f"- **å½’æ¡£æ€»æ•°**: {total_archives}\n")
        f.write(f"- **å ç”¨ç©ºé—´**: {total_size // 1024} MB\n\n")

        # å¼‚å¸¸æŠ¥å‘Š
        if has_alerts:
            f.write("## âš ï¸ éœ€è¦å…³æ³¨çš„ä»“åº“\n\n")
            f.write(
                "ä»¥ä¸‹ä»“åº“æ£€æµ‹åˆ°æäº¤æ•°æˆ–å¤§å°å¼‚å¸¸å‡å°‘ï¼Œå¯èƒ½å‘ç”Ÿäº† force push æˆ–å†å²é‡å†™ï¼š\n\n"
            )

            reviewed_repos = set()
            for line in (backup_root / ".need_review").read_text().splitlines():
                repo_name = line.strip()
                if not repo_name or repo_name in reviewed_repos:
                    continue
                reviewed_repos.add(repo_name)

                alert_file = backup_root / repo_name / ".alerts"
                if alert_file.exists():
                    f.write(f"### {repo_name}\n")
                    f.write("```\n")
                    # åªæ˜¾ç¤ºæœ€å20è¡Œ
                    alerts = alert_file.read_text().splitlines()
                    f.write('\n'.join(alerts[-20:]))
                    f.write("\n```\n\n")

                    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
                    commit_tracking_file = backup_root / repo_name / ".commit_tracking"
                    size_tracking_file = backup_root / repo_name / ".size_tracking"

                    current_info = []
                    if commit_tracking_file.exists():
                        try:
                            commits = commit_tracking_file.read_text().strip()
                            current_info.append(f"å½“å‰æäº¤æ•°: {commits}")
                        except Exception:
                            pass
                    if size_tracking_file.exists():
                        try:
                            size_kb = int(size_tracking_file.read_text().strip())
                            size_mb = size_kb // 1024
                            current_info.append(f"å½“å‰å¤§å°: {size_mb}MB")
                        except Exception:
                            pass

                    if current_info:
                        f.write(f"**å½“å‰çŠ¶æ€**: {' | '.join(current_info)}\n\n")

                    # å—ä¿æŠ¤çš„å¿«ç…§
                    snapshot_dir = backup_root / repo_name / "snapshots"
                    if snapshot_dir.exists():
                        snapshots = sorted(
                            snapshot_dir.iterdir(),
                            key=lambda x: x.stat().st_mtime,
                            reverse=True,
                        )
                        latest = snapshots[0].name if snapshots else "æ— "
                        f.write(f"**æœ€æ–°å¿«ç…§**: {latest}\n")

                        # åˆ—å‡ºå—ä¿æŠ¤çš„å¿«ç…§
                        protected_snapshots = [
                            s.name
                            for s in snapshots
                            if s.is_dir() and (s / ".protected").exists()
                        ]
                        if protected_snapshots:
                            f.write(
                                f"**ğŸ”’ å—ä¿æŠ¤å¿«ç…§** ({len(protected_snapshots)}ä¸ªï¼Œæ°¸ä¹…ä¿ç•™):\n"
                            )
                            for ps in protected_snapshots[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                f.write(f"  - {ps}\n")
                            if len(protected_snapshots) > 5:
                                f.write(
                                    f"  - ... è¿˜æœ‰ {len(protected_snapshots) - 5} ä¸ª\n"
                                )
                        f.write("\n")

                    f.write("**æ¢å¤å‘½ä»¤**:\n")
                    f.write("```bash\n")
                    f.write(f"{backup_root}/{repo_name}/restore.sh\n")
                    f.write("```\n\n")
                    f.write("---\n\n")

            # æ¸…ç©ºå¾…å®¡æ ¸åˆ—è¡¨
            (backup_root / ".need_review").unlink()
        else:
            f.write("## âœ… å…¨éƒ¨æ­£å¸¸\n\n")
            f.write("æœ¬å‘¨æœŸå†…æ‰€æœ‰ä»“åº“å‡æœªæ£€æµ‹åˆ°å¼‚å¸¸ã€‚\n\n")

        # æäº¤æ•°å˜åŒ–ç»Ÿè®¡
        repos_with_changes = [r for r in repo_details if r['commit_change']]
        if repos_with_changes:
            f.write("## ğŸ“ˆ æäº¤æ•°å˜åŒ–è®°å½•\n\n")
            f.write("| ä»“åº“ | å½“å‰æäº¤æ•° | å˜åŒ–æƒ…å†µ |\n")
            f.write("|------|-----------|----------|\n")
            for repo in repos_with_changes:
                f.write(
                    f"| {repo['name']} | {repo['commits']} | {repo['commit_change']} |\n"
                )
            f.write("\n")

        # ä»“åº“è¯¦æƒ…
        f.write("## ğŸ“¦ ä»“åº“å¤‡ä»½è¯¦æƒ…\n\n")
        f.write(
            "| ä»“åº“ | æäº¤æ•° | å¿«ç…§æ•° | å—ä¿æŠ¤ | æœ€æ–°å¿«ç…§ | å½’æ¡£æ•° | å ç”¨ç©ºé—´ | çŠ¶æ€ |\n"
        )
        f.write(
            "|------|--------|--------|--------|----------|--------|----------|------|\n"
        )

        for repo in repo_details:
            size_mb = repo['size_kb'] // 1024

            # çŠ¶æ€æ ‡è¯†
            status = "âœ…"
            if repo['commit_change']:
                status = "âš ï¸ æäº¤æ•°å‡å°‘"

            # å—ä¿æŠ¤å¿«ç…§æ˜¾ç¤º
            protected_display = (
                f"ğŸ”’ {repo['protected_snapshots']}"
                if repo['protected_snapshots'] > 0
                else "-"
            )

            f.write(
                f"| {repo['name']} | {repo['commits']} | {repo['snapshot_count']} | {protected_display} | "
                f"{repo['latest_snapshot']} | {repo['archive_count']} | {size_mb}MB | {status} |\n"
            )

        # ç£ç›˜ä½¿ç”¨æƒ…å†µ
        f.write("\n## ğŸ’¾ ç£ç›˜ä½¿ç”¨æƒ…å†µ\n\n")
        try:
            df_result = run_command(['df', '-h', str(backup_root)])
            lines = df_result.stdout.strip().split('\n')
            if len(lines) > 1:
                parts = lines[1].split()
                f.write(f"- **åˆ†åŒº**: {parts[0]}\n")
                f.write(f"- **æ€»ç©ºé—´**: {parts[1]}\n")
                f.write(f"- **å·²ç”¨**: {parts[2]} ({parts[4]})\n")
                f.write(f"- **å¯ç”¨**: {parts[3]}\n")
        except Exception as e:
            logger.warning(f"è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")

        f.write("\n---\n\n")
        f.write("**è¯´æ˜**:\n")
        f.write("- å¿«ç…§ä½¿ç”¨ç¡¬é“¾æ¥æŠ€æœ¯ï¼Œå®é™…å ç”¨ç©ºé—´è¿œå°äºæ˜¾ç¤ºå€¼\n")
        f.write(
            "- æŠ¥å‘Šæ¯æ¬¡å¤‡ä»½æ—¶è‡ªåŠ¨ç”Ÿæˆï¼Œå†å²æŠ¥å‘Šä¿ç•™ {} å¤©\n".format(
                config.REPORT_RETENTION_DAYS
            )
        )
        f.write("- ğŸ”’ æ£€æµ‹åˆ°å¼‚å¸¸æ—¶ï¼Œå¯¹åº”çš„**å¿«ç…§å’ŒæŠ¥å‘Š**ä¼šè‡ªåŠ¨æ ‡è®°ä¸º**æ°¸ä¹…ä¿ç•™**\n")
        f.write("- å¦‚éœ€æ¢å¤ä»“åº“ï¼Œä½¿ç”¨å¯¹åº”çš„ restore.sh è„šæœ¬\n")
        f.write(f"- æœ€æ–°æŠ¥å‘Šé“¾æ¥: {config.LATEST_REPORT}\n")
        f.write("\n")
        f.write("**å—ä¿æŠ¤èµ„æºç®¡ç†**:\n")
        f.write("- æŸ¥çœ‹å¿«ç…§ä¿æŠ¤: `cat /path/to/snapshot/.protected`\n")
        f.write("- æŸ¥çœ‹æŠ¥å‘Šä¿æŠ¤: `cat /path/to/report-xxx.md.protected`\n")
        f.write("- å–æ¶ˆä¿æŠ¤: åˆ é™¤å¯¹åº”çš„ `.protected` æ–‡ä»¶\n")
        f.write("- å†æ¬¡è¿è¡Œå¤‡ä»½åï¼Œè¶…æœŸèµ„æºå°†è¢«æ¸…ç†\n")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿æŠ¤æ­¤æŠ¥å‘Šï¼ˆå·²ç»åœ¨å‰é¢æ£€æŸ¥è¿‡äº†ï¼‰
    if has_alerts:
        # æœ‰éœ€è¦å…³æ³¨çš„ä»“åº“ï¼Œä¿æŠ¤æ­¤æŠ¥å‘Š
        protect_file = report_file.with_suffix('.md.protected')
        try:
            with open(protect_file, 'w') as f:
                f.write("# æ­¤æŠ¥å‘Šå·²è¢«æ ‡è®°ä¸ºæ°¸ä¹…ä¿ç•™\n")
                f.write("# ä¿æŠ¤åŸå› : åŒ…å«å¼‚å¸¸æ£€æµ‹è®°å½•\n")
                f.write(f"# æ ‡è®°æ—¶é—´: {datetime.now().isoformat()}\n")
                f.write("#\n")
                f.write("# æ­¤æŠ¥å‘Šè®°å½•äº†ä»“åº“å¼‚å¸¸ï¼Œä¸å—ä¿æŠ¤çš„å¿«ç…§ç›¸å¯¹åº”\n")
                f.write("# å¦‚éœ€å–æ¶ˆä¿æŠ¤ï¼Œåˆ é™¤æ­¤æ–‡ä»¶å³å¯\n")
            logger.info("ğŸ”’ æŠ¥å‘Šå·²æ ‡è®°ä¸ºæ°¸ä¹…ä¿ç•™ï¼ˆåŒ…å«å¼‚å¸¸è®°å½•ï¼‰")
        except Exception as e:
            logger.warning(f"æ ‡è®°æŠ¥å‘Šä¿æŠ¤å¤±è´¥: {e}")

    # åˆ›å»ºæˆ–æ›´æ–°åˆ°æœ€æ–°æŠ¥å‘Šçš„è½¯é“¾æ¥ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    try:
        if latest_report.exists() or latest_report.is_symlink():
            latest_report.unlink()
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„åˆ›å»ºè½¯é“¾æ¥
        relative_path = report_file.relative_to(latest_report.parent)
        latest_report.symlink_to(relative_path)
        logger.info(f"âœ“ æŠ¥å‘Šç”Ÿæˆ: {report_file}")
        logger.info(f"âœ“ æœ€æ–°æŠ¥å‘Š: {latest_report}")
    except Exception as e:
        logger.warning(f"åˆ›å»ºè½¯é“¾æ¥å¤±è´¥: {e}")
        logger.info(f"âœ“ æŠ¥å‘Šç”Ÿæˆ: {report_file}")


# ============ ä¸»å‡½æ•° ============
def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("Gitea Docker é•œåƒå¤‡ä»½ä»»åŠ¡å¼€å§‹")
    logger.info("=" * 50)

    # æ£€æŸ¥ Docker
    if not check_docker_container():
        sys.exit(1)

    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    backup_root = Path(config.BACKUP_ROOT)
    backup_root.mkdir(parents=True, exist_ok=True)

    # è·å–ä»“åº“è·¯å¾„
    repos_path = Path(config.GITEA_DATA_VOLUME) / config.GITEA_REPOS_PATH

    if not repos_path.exists():
        logger.error(f"ä»“åº“ç›®å½•ä¸å­˜åœ¨: {repos_path}")
        sys.exit(1)

    logger.info(f"ä»“åº“ç›®å½•: {repos_path}")

    # åˆ—å‡ºç›®å½•å†…å®¹ä»¥ä¾¿è°ƒè¯•
    logger.info("æ‰«æç»„ç»‡ç›®å½•...")
    org_dirs = [d for d in repos_path.iterdir() if d.is_dir()]
    logger.info(f"æ‰¾åˆ° {len(org_dirs)} ä¸ªç»„ç»‡ç›®å½•: {[d.name for d in org_dirs]}")

    # å¤„ç†æ‰€æœ‰ä»“åº“
    processed_count = 0
    skipped_count = 0

    for org_dir in repos_path.iterdir():
        if not org_dir.is_dir():
            continue

        logger.info(f"æ£€æŸ¥ç»„ç»‡: {org_dir.name}")

        # æŸ¥æ‰¾æ‰€æœ‰ .git ç›®å½•
        git_repos = list(org_dir.glob("*.git"))
        logger.info(f"  æ‰¾åˆ° {len(git_repos)} ä¸ª .git ä»“åº“")

        for repo_path in git_repos:
            if not repo_path.is_dir():
                continue

            try:
                backup = RepositoryBackup(repo_path)
                logger.info(f"  æ£€æŸ¥ä»“åº“: {backup.full_name}")

                if not backup.should_backup():
                    logger.info(f"  è·³è¿‡: {backup.full_name}")
                    skipped_count += 1
                    continue

                backup.process()
                processed_count += 1

            except Exception as e:
                logger.error(f"å¤„ç†ä»“åº“å¤±è´¥ {repo_path}: {e}", exc_info=True)

    logger.info(f"è·³è¿‡äº† {skipped_count} ä¸ªä»“åº“")

    logger.info("=" * 50)
    logger.info(f"å¤„ç†äº† {processed_count} ä¸ªä»“åº“")

    # æ¯æ¬¡éƒ½ç”ŸæˆæŠ¥å‘Š
    generate_report()

    # æ¸…ç†æ—§æŠ¥å‘Š
    cleanup_old_reports()

    # å‘é€é€šçŸ¥
    if notifier:
        try:
            send_backup_notification(processed_count, skipped_count)
        except Exception as e:
            logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")

    logger.info("=" * 50)
    logger.info("å¤‡ä»½ä»»åŠ¡å®Œæˆ")
    logger.info("=" * 50)


if __name__ == "__main__":
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(
            description='Gitea Docker é•œåƒä»“åº“å¤‡ä»½ç³»ç»Ÿ',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ç¤ºä¾‹:
  %(prog)s                          # æ‰§è¡Œå®Œæ•´å¤‡ä»½
  %(prog)s -c config.yaml           # ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶
  %(prog)s --report                 # åªç”ŸæˆæŠ¥å‘Š
  %(prog)s --cleanup                # åªæ¸…ç†æ—§æŠ¥å‘Š
  %(prog)s --show-config            # æ˜¾ç¤ºå½“å‰é…ç½®
  %(prog)s --validate-config        # éªŒè¯é…ç½®æ–‡ä»¶

ç¯å¢ƒå˜é‡:
  GITEA_DOCKER_CONTAINER            # Docker å®¹å™¨åç§°
  BACKUP_ROOT                       # å¤‡ä»½æ ¹ç›®å½•
  BACKUP_ORGANIZATIONS              # å¤‡ä»½ç»„ç»‡ï¼ˆé€—å·åˆ†éš”ï¼‰
  æ›´å¤šç¯å¢ƒå˜é‡è¯·å‚è€ƒæ–‡æ¡£
            """,
        )

        parser.add_argument('-c', '--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config.yamlï¼‰')
        parser.add_argument(
            '--report', action='store_true', help='åªç”ŸæˆæŠ¥å‘Šï¼Œä¸æ‰§è¡Œå¤‡ä»½'
        )
        parser.add_argument('--cleanup', action='store_true', help='åªæ¸…ç†æ—§æŠ¥å‘Š')
        parser.add_argument('--show-config', action='store_true', help='æ˜¾ç¤ºå½“å‰é…ç½®')
        parser.add_argument(
            '--validate-config', action='store_true', help='éªŒè¯é…ç½®æ–‡ä»¶'
        )

        args = parser.parse_args()

        # åˆå§‹åŒ–é…ç½®
        Config.init(args.config)
        config = Config()

        # åˆå§‹åŒ–æ—¥å¿—
        logger = setup_logging(config)

        # åˆå§‹åŒ–é€šçŸ¥ç³»ç»Ÿ
        if NOTIFIER_AVAILABLE:
            try:
                notifier = NotificationManager(config)
                logger.info("é€šçŸ¥ç³»ç»Ÿå·²åˆå§‹åŒ–")
            except Exception as e:
                logger.warning(f"é€šçŸ¥ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                notifier = None
        else:
            notifier = None
            logger.info("é€šçŸ¥ç³»ç»Ÿä¸å¯ç”¨ï¼ˆæœªå®‰è£… requests åº“ï¼‰")

        # æ˜¾ç¤ºé…ç½®
        if args.show_config:
            config.get_loader().print_config()
            sys.exit(0)

        # éªŒè¯é…ç½®
        if args.validate_config:
            errors = config.get_loader().validate()
            if errors:
                print("\né…ç½®é”™è¯¯:")
                for error in errors:
                    print(f"  âœ— {error}")
                sys.exit(1)
            else:
                print("\nâœ“ é…ç½®éªŒè¯é€šè¿‡")
                sys.exit(0)

        # åªç”ŸæˆæŠ¥å‘Š
        if args.report:
            logger.info("æ‰‹åŠ¨ç”ŸæˆæŠ¥å‘Š...")
            generate_report()
            sys.exit(0)

        # åªæ¸…ç†æ—§æŠ¥å‘Š
        if args.cleanup:
            logger.info("æ¸…ç†æ—§æŠ¥å‘Š...")
            cleanup_old_reports()
            sys.exit(0)

        # æ‰§è¡Œå®Œæ•´å¤‡ä»½
        main()

    except KeyboardInterrupt:
        if logger:
            logger.info("\nä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        if logger:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        else:
            print(f"é”™è¯¯: {e}")
        sys.exit(1)
